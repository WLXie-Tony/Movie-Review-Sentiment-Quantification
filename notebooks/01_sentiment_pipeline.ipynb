{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd89b660-20d3-442f-8a91-ffce6b7b7666",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Pipeline for Unstructured Textual Data\n",
    "\n",
    "**Author:** Wenlan (Tony) Xie  \n",
    "**Affiliation:** The University of Sydney  \n",
    "**Contact:** wxie3035@uni.sydney.edu.au  \n",
    "**Last Updated:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ Associated Publication\n",
    "This codebase constitutes the core data processing pipeline for the following peer-reviewed article:\n",
    "\n",
    "> **Tian, H., Xie, W. T., & Zhang, Y. (2026).** \"Reading Between the Reels: An AI-Driven Approach to Analysing Movie Review Sentiment and Market Returns.\" *International Journal of Finance & Economics*.  \n",
    "> **Status:** Accepted / Published  \n",
    "> **DOI:** [10.1002/ijfe.70129](https://onlinelibrary.wiley.com/doi/10.1002/ijfe.70129)\n",
    "\n",
    "### üìå Abstract\n",
    "This notebook demonstrates a production-grade **ETL (Extract, Transform, Load) pipeline** designed to process large-scale unstructured textual data (approx. 250k observations) for econometric analysis. The pipeline utilizes **Large Language Models (GPT-4o)** to extract high-dimensional sentiment signals, which serve as the primary independent variable in the associated asset pricing study.\n",
    "\n",
    "### üõ† Key Technical Features\n",
    "To ensure **scalability**, **reproducibility**, and **data integrity** suitable for high-dimensional econometric analysis, this implementation incorporates:\n",
    "\n",
    "* **Asynchronous Concurrency (Event Loop Optimization):** Implements a semaphore-controlled `asyncio` event loop to manage high-throughput API requests, effectively handling I/O blocking and reducing processing latency by $\\approx 95\\%$ compared to synchronous execution.\n",
    "* **Deterministic Schema Enforcement:** Utilizes `Pydantic` to enforce rigid data typing on stochastic LLM outputs, preventing parsing errors and ensuring strict conformity to the defined data schema for downstream regression tasks.\n",
    "* **Resilience & Error Handling:** Deploys a **Truncated Binary Exponential Backoff** strategy (via `tenacity`) to robustly handle transient API instability and rate limit (HTTP 429) signals.\n",
    "* **Idempotency & State Management:** Supports resumable execution protocols, preventing data loss and redundant computation costs in the event of interrupt signals or system failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd3c69-f380-490a-9fb0-1cd0fff70674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System & Configuration\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# API & Networking\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Resilience & Validation\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Visualization\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore') # Suppress non-critical warnings\n",
    "load_dotenv() # Securely load API keys from .env file\n",
    "\n",
    "# Configure Logging to display process flow clearly\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(\"ResearchPipeline\")\n",
    "\n",
    "print(\"‚úÖ Environment Configured. Libraries Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422e020-9abc-41f1-a197-3430f68bace6",
   "metadata": {},
   "source": [
    "### 3. Data Schema & Cost Management\n",
    "\n",
    "To guarantee that the unstructured text is converted into a structured format usable for Stata/Python regressions, we define a strict `Pydantic` schema.\n",
    "\n",
    "**Cost Estimation Logic:**\n",
    "The pipeline tracks token usage in real-time to manage research budgets. The cost $C$ for a request is calculated as:\n",
    "\n",
    "$$C = (N_{input} \\times P_{input}) + (N_{output} \\times P_{output})$$\n",
    "\n",
    "Where $P$ represents the price per 1k tokens for the specific model version (e.g., GPT-4o)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5310b-df12-4797-ac1f-e0d1d9f3d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    Strict Data Schema for LLM Output.\n",
    "    Enforces type constraints to ensure data integrity for econometric modeling.\n",
    "    \"\"\"\n",
    "    sentiment_score: int = Field(\n",
    "        ..., \n",
    "        ge=1, le=10, \n",
    "        description=\"Integer score from 1-10 (1=Extremely Negative, 10=Extremely Positive)\"\n",
    "    )\n",
    "    emotion_keywords: List[str] = Field(\n",
    "        ..., \n",
    "        min_items=1, max_items=5, \n",
    "        description=\"List of 1-5 keywords representing emotional tone\"\n",
    "    )\n",
    "    primary_emotion: str = Field(..., description=\"Dominant emotion identified in the text\")\n",
    "    review_focus: str = Field(..., description=\"Thematic focus (e.g., Plot, Acting, Cinematography)\")\n",
    "    bias_analysis: str = Field(..., description=\"Assessment of potential reviewer bias\")\n",
    "    summary: str = Field(..., description=\"Concise summary (<50 words)\")\n",
    "\n",
    "print(\"‚úÖ Data Schema Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f2682-ef1e-4ca9-9548-26eddd0e39e3",
   "metadata": {},
   "source": [
    "### 3.1 Mathematical Formulation of the Extraction Framework\n",
    "\n",
    "To rigorously quantify the unstructured qualitative information embedded in movie reviews, we formalize the LLM-based extraction process as a probabilistic mapping and optimization problem.\n",
    "\n",
    "#### A. Sentiment Extraction Function\n",
    "Let $\\mathcal{D} = \\{ (T_i, \\mathbf{X}_i) \\}_{i=1}^N$ denote the dataset of $N$ movie reviews, where $T_i$ represents the raw text of review $i$, and $\\mathbf{X}_i \\in \\mathbb{R}^d$ represents the vector of associated metadata (e.g., box office, budget, director).We define the Large Language Model as a parameterized conditional probability distribution $P_{\\theta}(\\cdot)$. The extraction process for a specific review $i$ is modeled as drawing a realization $\\mathcal{S}_i$ from the posterior distribution:$$\\mathcal{S}_i \\sim P_{\\theta}(\\mathcal{S} \\mid T_i \\oplus \\mathbf{X}_i, \\mathcal{P}; \\tau)$$Where:$\\mathcal{P}$ is the structured system prompt imposing domain-specific constraints (e.g., Persona: Financial Critic).$\\tau$ denotes the temperature hyperparameter. We set $\\tau=0.2$ to minimize the entropy $H(P_\\theta)$, thereby reducing stochastic variation $\\sigma^2$ and ensuring asymptotic reproducibility.$\\mathcal{S}_i$ is the resulting structured tensor containing the sentiment scalar $s_i \\in [1, 10]$ and the extracted feature vector $\\mathbf{e}_i$.$\\oplus$ denotes the textual concatenation operator.\n",
    "\n",
    "#### B. Cost Estimation Function\n",
    "Given the high-volume nature of the pipeline ($N \\approx 2.5 \\times 10^5$), cost modeling is critical. Let $\\mathcal{T}(\\cdot)$ denote the tokenizer function (specifically cl100k_base for GPT-4o) mapping string space to token space $\\mathbb{Z}^*$. The total cost objective function $C_{total}$ is defined as:$$C_{total} = \\sum_{i=1}^{N} \\left[ \\frac{|\\mathcal{T}(T_i \\oplus \\mathbf{X}_i \\oplus \\mathcal{P})|}{1000} \\cdot \\lambda_{in} + \\frac{|\\mathcal{T}(\\mathcal{S}_i)|}{1000} \\cdot \\lambda_{out} \\right]$$Where:$|\\cdot|$ denotes the cardinality (length) of the token sequence.$\\lambda_{in}$ and $\\lambda_{out}$ represent the marginal cost per 1,000 tokens for input and output contexts, respectively.\n",
    "\n",
    "#### C. Robustness via Exponential Backoff\n",
    "To adhere to API rate limits (HTTP 429), we implement a Truncated Binary Exponential Backoff algorithm with Full Jitter. The wait time $W_k$ for the $k$-th retry attempt is defined as:$$W_k = \\min(W_{cap}, W_{base} \\cdot 2^k) + \\epsilon$$Where:$W_{cap}$ is the maximum allowable latency (ceiling).$W_{base}$ is the initial backoff interval.$\\epsilon \\sim U(0, 1)$ represents a random jitter term (in seconds) introduced to decorrelate concurrent requests and prevent the \"thundering herd\" phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c81eb-16b5-4164-b84e-9e101c9960d6",
   "metadata": {},
   "source": [
    "### 4. Asynchronous Pipeline Implementation\n",
    "\n",
    "The `MovieReviewResearcher` class encapsulates the core logic. It utilizes a **Semaphore** pattern to limit concurrency (avoiding HTTP 429 errors) and utilizes `tenacity` decorators for robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c49f52-5277-4252-abb6-9e339b296f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Asynchronous Pipeline Implementation ---\n",
    "\n",
    "# Configuration Management (Best Practice: Keep constants separate)\n",
    "class PipelineConfig:\n",
    "    MODEL_NAME: str = \"gpt-4o-2024-05-13\"  # Pinning version for reproducibility\n",
    "    MAX_CONCURRENCY: int = 20            # Semaphore limit to avoid hitting strict TPM limits\n",
    "    TEMPERATURE: float = 0.2             # Low temperature for reduced stochasticity\n",
    "    MAX_RETRIES: int = 5                 # Robustness factor\n",
    "    # Pricing per 1k tokens (Update based on current OpenAI pricing)\n",
    "    COST_INPUT_PER_1K: float = 0.0050\n",
    "    COST_OUTPUT_PER_1K: float = 0.0150\n",
    "\n",
    "class MovieReviewResearcher:\n",
    "    \"\"\"\n",
    "    Asynchronous ETL pipeline for extracting sentiment signals from unstructured text.\n",
    "    Encapsulates logic for rate-limiting, cost tracking, and failure recovery.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_file: str, output_dir: str):\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir\n",
    "        self.total_cost = 0.0\n",
    "        \n",
    "        # Thread-safe locks and semaphores for async context\n",
    "        self.cost_lock = asyncio.Lock()\n",
    "        self.sem = asyncio.Semaphore(PipelineConfig.MAX_CONCURRENCY)\n",
    "        \n",
    "        # Initialize Tokenizer for precise cost estimation\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "        except KeyError:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            \n",
    "        # Async Client Initialization\n",
    "        self.client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def _estimate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:\n",
    "        \"\"\"Calculates precise request cost based on token usage.\"\"\"\n",
    "        input_cost = (prompt_tokens / 1000) * PipelineConfig.COST_INPUT_PER_1K\n",
    "        output_cost = (completion_tokens / 1000) * PipelineConfig.COST_OUTPUT_PER_1K\n",
    "        return input_cost + output_cost\n",
    "\n",
    "    @retry(\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=60), # Exponential Backoff\n",
    "        stop=stop_after_attempt(PipelineConfig.MAX_RETRIES),\n",
    "        retry=retry_if_exception_type((Exception)), \n",
    "        reraise=True\n",
    "    )\n",
    "    async def _analyze_single_row(self, idx: int, row: pd.Series) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Core atomic operation: Semantic extraction for a single record.\n",
    "        Includes semaphore context management and strict schema validation.\n",
    "        \"\"\"\n",
    "        async with self.sem:  # Acquire semaphore slot\n",
    "            try:\n",
    "                # 1. Construct Domain-Specific System Prompt\n",
    "                system_prompt = (\n",
    "                    \"You are an expert econometrician and film critic. \"\n",
    "                    \"Analyze the following movie review to extract structured sentiment data \"\n",
    "                    \"for academic research. adhere strictly to the JSON schema.\"\n",
    "                )\n",
    "                \n",
    "                # 2. Contextual User Input\n",
    "                user_content = f\"\"\"\n",
    "                Metadata:\n",
    "                - Title: {row.get('Title', 'Unknown')}\n",
    "                - Director: {row.get('Director', 'Unknown')}\n",
    "                - Budget: ${row.get('Budget', 0):,}\n",
    "                \n",
    "                Review Text:\n",
    "                '''{row.get('Comments', '')}'''\n",
    "                \"\"\"\n",
    "\n",
    "                # 3. LLM Inference (GPT-4o JSON Mode)\n",
    "                response = await self.client.chat.completions.create(\n",
    "                    model=PipelineConfig.MODEL_NAME,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_content}\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"}, # Enforce Valid JSON\n",
    "                    temperature=PipelineConfig.TEMPERATURE\n",
    "                )\n",
    "\n",
    "                # 4. Parsing & Validation\n",
    "                raw_json = response.choices[0].message.content\n",
    "                usage = response.usage\n",
    "                \n",
    "                # Pydantic Validation: Throws ValidationError if schema is violated\n",
    "                parsed_data = ReviewAnalysis.model_validate_json(raw_json)\n",
    "\n",
    "                # 5. Cost Accumulation (Thread-safe)\n",
    "                cost = self._estimate_cost(usage.prompt_tokens, usage.completion_tokens)\n",
    "                async with self.cost_lock:\n",
    "                    self.total_cost += cost\n",
    "\n",
    "                # 6. Return Enriched Record\n",
    "                return {\n",
    "                    \"original_index\": idx,\n",
    "                    **row.to_dict(),\n",
    "                    **parsed_data.model_dump(),\n",
    "                    \"request_cost\": round(cost, 6),\n",
    "                    \"prompt_tokens\": usage.prompt_tokens,\n",
    "                    \"completion_tokens\": usage.completion_tokens,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                # Logging failure for post-mortem analysis\n",
    "                logger.error(f\"Error processing index {idx}: {str(e)}\")\n",
    "                raise e # Trigger retry logic\n",
    "\n",
    "    async def run_pipeline(self, sample_size: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Orchestrator function: Handles data loading, batch processing, and idempotent saving.\n",
    "        \"\"\"\n",
    "        # 1. Load and Preprocess Data\n",
    "        if self.input_file.endswith('.xlsx'):\n",
    "            df = pd.read_excel(self.input_file)\n",
    "        else:\n",
    "            df = pd.read_csv(self.input_file)\n",
    "            \n",
    "        logger.info(f\"Loaded {len(df)} records from {self.input_file}\")\n",
    "        \n",
    "        if sample_size:\n",
    "            df = df.head(sample_size)\n",
    "            logger.info(f\"Subsampling first {sample_size} records for testing.\")\n",
    "\n",
    "        # 2. Idempotency Check (Skip already processed rows)\n",
    "        output_file = os.path.join(self.output_dir, \"analysis_results_master.csv\")\n",
    "        processed_indices = set()\n",
    "        \n",
    "        if os.path.exists(output_file):\n",
    "            try:\n",
    "                # Check existing output to resume progress\n",
    "                existing_df = pd.read_csv(output_file)\n",
    "                if 'original_index' in existing_df.columns:\n",
    "                    processed_indices = set(existing_df['original_index'].unique())\n",
    "                    logger.info(f\"Resuming: Found {len(processed_indices)} processed records.\")\n",
    "            except Exception:\n",
    "                logger.warning(\"Output file unreadable or empty. Starting fresh.\")\n",
    "\n",
    "        # 3. Task Generation\n",
    "        tasks = []\n",
    "        rows_to_process = []\n",
    "        for idx, row in df.iterrows():\n",
    "            if idx not in processed_indices:\n",
    "                tasks.append(self._analyze_single_row(idx, row))\n",
    "                rows_to_process.append(idx)\n",
    "\n",
    "        if not tasks:\n",
    "            logger.info(\"All records already processed. Pipeline complete.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Queueing {len(tasks)} tasks with concurrency limit {PipelineConfig.MAX_CONCURRENCY}...\")\n",
    "\n",
    "        # 4. Batch Execution & Incremental Saving\n",
    "        # We save incrementally to prevent memory overflow and data loss\n",
    "        batch_size = 50 \n",
    "        \n",
    "        # Using tqdm for progress visualization\n",
    "        results = []\n",
    "        for i in range(0, len(tasks), batch_size):\n",
    "            batch = tasks[i : i + batch_size]\n",
    "            \n",
    "            # Run batch concurrently\n",
    "            batch_results = await tqdm.gather(*batch, desc=f\"Processing Batch {i//batch_size + 1}\")\n",
    "            \n",
    "            # Filter failed results (None)\n",
    "            valid_results = [r for r in batch_results if r is not None]\n",
    "            \n",
    "            if valid_results:\n",
    "                temp_df = pd.DataFrame(valid_results)\n",
    "                # Append to CSV\n",
    "                temp_df.to_csv(\n",
    "                    output_file, \n",
    "                    mode='a', \n",
    "                    header=not os.path.exists(output_file), \n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "            logger.info(f\"Batch {i//batch_size + 1} saved. Cumulative Cost: ${self.total_cost:.4f}\")\n",
    "\n",
    "        logger.info(\"‚úÖ Pipeline Execution Finished Successfully.\")\n",
    "        logger.info(f\"Final Estimated Cost: ${self.total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9366b93-2417-47d0-9993-f1bae4aa7c00",
   "metadata": {},
   "source": [
    "### 5. Execution & Demonstration\n",
    "\n",
    "For reproducibility purposes, this section generates a **synthetic dataset** to demonstrate the pipeline's functionality without requiring external dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b8736-ffab-44aa-aad2-4d5fca5325a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_demo():\n",
    "    \"\"\"\n",
    "    Generates synthetic data and runs the pipeline for demonstration.\n",
    "    \"\"\"\n",
    "    # 1. Create Synthetic Data (Mocking the Excel file)\n",
    "    mock_data = {\n",
    "        'Title': ['Inception', 'The Room', 'Godfather'],\n",
    "        'Director': ['Christopher Nolan', 'Tommy Wiseau', 'Francis Ford Coppola'],\n",
    "        'Budget': [160000000, 6000000, 6000000],\n",
    "        'Gross_Worldwide': [836800000, 4993000, 246100000],\n",
    "        'Comments': [\n",
    "            \"A masterpiece of mind-bending visuals and storytelling. Nolan is a genius.\",\n",
    "            \"This is unironically the worst movie I have ever seen. The acting is wooden.\",\n",
    "            \"An offer you can't refuse. Absolute cinema perfection.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_mock = pd.DataFrame(mock_data)\n",
    "    input_file = \"demo_dataset.xlsx\"\n",
    "    df_mock.to_excel(input_file, index=False)\n",
    "    \n",
    "    print(f\"üìä Created synthetic dataset with {len(df_mock)} records.\")\n",
    "\n",
    "    # 2. Initialize Researcher\n",
    "    # Note: Ensure OPENAI_API_KEY is set in your environment\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"‚ö†Ô∏è No API Key found. Skipping actual API call for safety.\")\n",
    "        return\n",
    "\n",
    "    researcher = MovieReviewResearcher(input_file=input_file, output_dir=\"./demo_results\")\n",
    "    \n",
    "    # 3. Run Pipeline\n",
    "    await researcher.run_pipeline()\n",
    "    \n",
    "    # 4. Display Results\n",
    "    result_file = \"./demo_results/analysis_results_master.csv\"\n",
    "    if os.path.exists(result_file):\n",
    "        df_result = pd.read_csv(result_file)\n",
    "        print(\"\\nüèÜ Analysis Results Preview:\")\n",
    "        display(df_result[['Title', 'sentiment_score', 'primary_emotion', 'request_cost']])\n",
    "    else:\n",
    "        print(\"No results generated.\")\n",
    "\n",
    "# Run the async loop in Jupyter\n",
    "await run_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
